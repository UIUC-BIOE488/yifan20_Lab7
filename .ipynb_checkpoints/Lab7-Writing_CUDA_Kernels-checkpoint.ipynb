{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing CUDA Kernels\n",
    "-------------------\n",
    "## Contents\n",
    "- Example - computing pairwise distances given a dataset of points\n",
    "    - Solution using serial for-loops\n",
    "    - Simple CUDA solution\n",
    "    - Optimized CUDA solution\n",
    "- Example - matrix multiplication\n",
    "    - Simple CUDA solution\n",
    "    - Optimized CUDA solution\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the GPU allocated to us using the ```nvidia-smi``` shell command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov  6 18:18:33 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.107.02             Driver Version: 550.107.02     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   22C    P8             21W /  300W |      11MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               On  |   00000000:23:00.0 Off |                  Off |\n",
      "| 30%   22C    P8             15W /  300W |      11MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000               On  |   00000000:41:00.0 Off |                  Off |\n",
      "| 30%   21C    P8             17W /  300W |      11MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000               On  |   00000000:61:00.0 Off |                  Off |\n",
      "| 30%   21C    P8             18W /  300W |      11MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000               On  |   00000000:81:00.0 Off |                  Off |\n",
      "| 30%   22C    P8             29W /  300W |      11MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000               On  |   00000000:A1:00.0 Off |                  Off |\n",
      "| 30%   21C    P8             14W /  300W |      11MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA RTX A6000               On  |   00000000:C1:00.0 Off |                  Off |\n",
      "| 30%   22C    P8             21W /  300W |      11MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA RTX A6000               On  |   00000000:E1:00.0 Off |                  Off |\n",
      "| 30%   22C    P8             19W /  300W |      11MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    1   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    2   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    3   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    4   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    5   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    6   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    7   N/A  N/A      3212      G   /usr/libexec/Xorg                               4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CUDA devices\n",
      "id 0     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-671efd6e-627a-a4ac-7730-078545a0dfc3\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 1     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 35\n",
      "                                    UUID: GPU-8e053c2b-b3b2-adee-cb8e-3c63dcffa895\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 2     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 65\n",
      "                                    UUID: GPU-dc9172a8-f65e-8d98-369c-bdf8aa51c36e\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 3     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 97\n",
      "                                    UUID: GPU-c2101a36-db0e-28dd-913c-d4ddb6b70c3d\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 4     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 129\n",
      "                                    UUID: GPU-38fa86c0-3297-9c59-9d5b-bb1f5024927c\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 5     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 161\n",
      "                                    UUID: GPU-edecd529-76b3-1bc0-bf2f-fa5a381bbbe7\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 6     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 193\n",
      "                                    UUID: GPU-14a714b7-40e1-21d0-a1f6-aa2fd7afa84a\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 7     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 225\n",
      "                                    UUID: GPU-ba0658af-3d7e-f7c9-3643-4b68d53e6d65\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t8/8 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import cuda\n",
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, cuda, vectorize, guvectorize, stencil\n",
    "from numba import prange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import convolve as scipy_convolve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba.core.errors import NumbaPerformanceWarning\n",
    "# import warnings\n",
    "# warnings.simplefilter('ignore', category=NumbaPerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 1] Example - Computing Pairwise Distances\n",
    "\n",
    "This computation shows up in many practical scenarios/problems:\n",
    "- Distance matrices for clustering algorithms\n",
    "- k-Nearest Neighbors (kNN) classification algorithm\n",
    "- Estimation of topological manifolds\n",
    "- Similarity-based searches such as recommendations or information retrieval\n",
    "\n",
    "![](./distance_matrix.jpg)\n",
    "\n",
    "Notice that:\n",
    "- The cells of the distance matrix can be filled in parallel since the distance computation (Euclidean distance, for example) has no external dependencies\n",
    "- Typically, the distance between A and B is the same as B and A. Let's ignore this property for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "xy_coordinates = np.random.rand(n_samples, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14129753, 0.98950664],\n",
       "       [0.91683653, 0.00762487],\n",
       "       [0.91208837, 0.76367045],\n",
       "       ...,\n",
       "       [0.62972705, 0.22426763],\n",
       "       [0.99472208, 0.87769184],\n",
       "       [0.02805067, 0.11945782]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 2] Serial Solution using For-loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def distance_matrix_serial(data):\n",
    "    distance_matrix = np.zeros((data.shape[0], data.shape[0]))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[0]):\n",
    "            distance_matrix[i, j] = distance.euclidean(data[i], data[j])\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Serial] Starting the timer ---\n",
      "--- Done: The execution took 3.6154673099517822 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"--- [Serial] Starting the timer ---\")\n",
    "result_serial = distance_matrix_serial(xy_coordinates)\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 3] Simple CUDA Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# one block computes one cell inside the distance matrix\n",
    "# single thread inside the block performs the computation\n",
    "\n",
    "@cuda.jit\n",
    "def distance_matrix_cuda(data, distance_matrix):\n",
    "\n",
    "    i, j = cuda.grid(2)\n",
    "\n",
    "    if i > len(data) or j > len(data):\n",
    "        return\n",
    "    \n",
    "    distance_matrix[i, j] = ((data[i,0] - data[j,0])**2 + (data[i,1] - data[j,1])**2)**0.5\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [simple CUDA] Starting the timer ---\n",
      "--- Done: The execution took 0.016772747039794922 seconds ---\n",
      "\n",
      "Your result is correct!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/BIOE488/FA24/conda/rapids/lib/python3.12/site-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "cuda.select_device(2)  # Change to the device ID you want to use\n",
    "\n",
    "num_blocks = (n_samples, n_samples)\n",
    "num_threads_per_block = 1\n",
    "\n",
    "result_cuda = np.zeros((n_samples, n_samples), dtype=float)\n",
    "distance_matrix_cuda[num_blocks, num_threads_per_block](xy_coordinates, result_cuda); cuda.synchronize()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"\\n--- [simple CUDA] Starting the timer ---\")\n",
    "result_cuda = np.zeros((n_samples, n_samples), dtype=float)\n",
    "distance_matrix_cuda[num_blocks, num_threads_per_block](xy_coordinates, result_cuda); cuda.synchronize()\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"\\nYour result is {}\".format(\"correct!\" if np.allclose(result_serial, result_cuda) else \"incorrect.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 4] Simple CUDA Solution + Shared CUDA Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one block computes distances from current point to all other points\n",
    "# each point is handled by a separate thread inside the block\n",
    "# threads inside the block take the current point from shared memory of the block\n",
    "\n",
    "from numba import cuda, float64\n",
    "\n",
    "@cuda.jit\n",
    "def distance_matrix_cuda_optimized(data, distance_matrix):\n",
    "\n",
    "    current_point = cuda.shared.array(shape=(2), dtype=float64)\n",
    "\n",
    "    i = cuda.blockIdx.x\n",
    "    if i < data.shape[0]:\n",
    "        current_point = data[i]\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "\n",
    "    j = cuda.threadIdx.x\n",
    "    if j < data.shape[0]:\n",
    "        distance_matrix[i, j] = ((current_point[0] - data[j,0])**2 + (current_point[1] - data[j,1])**2) ** 0.5\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [simple CUDA + shared memory] Starting the timer ---\n",
      "--- Done: The execution took 0.003175020217895508 seconds ---\n",
      "\n",
      "Your result is correct!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/BIOE488/FA24/conda/rapids/lib/python3.12/site-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_blocks = n_samples\n",
    "num_threads_per_block = n_samples\n",
    "\n",
    "result_cuda_optimized = np.zeros((n_samples, n_samples), dtype=float)\n",
    "distance_matrix_cuda_optimized[num_blocks, num_threads_per_block](xy_coordinates, result_cuda_optimized); cuda.synchronize()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"\\n--- [simple CUDA + shared memory] Starting the timer ---\")\n",
    "result_cuda_optimized = np.zeros((n_samples, n_samples), dtype=float)\n",
    "distance_matrix_cuda_optimized[num_blocks, num_threads_per_block](xy_coordinates, result_cuda_optimized); cuda.synchronize()\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"\\nYour result is {}\".format(\"correct!\" if np.allclose(result_serial, result_cuda_optimized) else \"incorrect.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the difference with and without shared memory more clearly (100 repeats)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_simple_cuda(n_samples=1000, n_repeats=100):\n",
    "    exec_times = []\n",
    "    for _ in range(n_repeats):\n",
    "        xy_coordinates = np.random.rand(n_samples, 2)\n",
    "        num_blocks = (n_samples, n_samples)\n",
    "        num_threads_per_block = 1\n",
    "        result_cuda = np.zeros((n_samples, n_samples), dtype=float)\n",
    "        start_time = time.time()\n",
    "        distance_matrix_cuda[num_blocks, num_threads_per_block](xy_coordinates, result_cuda); cuda.synchronize()\n",
    "        exec_times.append(time.time() - start_time)\n",
    "    return exec_times\n",
    "\n",
    "def repeat_optimized_cuda(n_samples=1000, n_repeats=100):\n",
    "    exec_times = []\n",
    "    for _ in range(n_repeats):\n",
    "        xy_coordinates = np.random.rand(n_samples, 2)\n",
    "        num_blocks = n_samples\n",
    "        num_threads_per_block = n_samples\n",
    "        result_cuda_optimized = np.zeros((n_samples, n_samples), dtype=float)\n",
    "        start_time = time.time()\n",
    "        distance_matrix_cuda_optimized[num_blocks, num_threads_per_block](xy_coordinates, result_cuda_optimized); cuda.synchronize()\n",
    "        exec_times.append(time.time() - start_time)\n",
    "    return exec_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple CUDA (100 repeats): 0.012131321430206298 (0.0007611765860116238)\n",
      "Optimized CUDA (100 repeats): 0.0022139930725097656 (0.0006402132379021544)\n"
     ]
    }
   ],
   "source": [
    "exec_times_simple_cuda = repeat_simple_cuda()\n",
    "exec_times_optimized_cuda = repeat_optimized_cuda()\n",
    "\n",
    "print(f\"Simple CUDA (100 repeats): {np.mean(exec_times_simple_cuda)} ({np.std(exec_times_simple_cuda)})\")\n",
    "print(f\"Optimized CUDA (100 repeats): {np.mean(exec_times_optimized_cuda)} ({np.std(exec_times_optimized_cuda)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 5] Matrix multiplication - Simple CUDA Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each thread fills in one cell of the output matrix C\n",
    "\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]): # or range(B.shape[1])\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_blocks: (1600, 1600), num_threads_per_block: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/BIOE488/FA24/conda/rapids/lib/python3.12/site-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [simple CUDA] Starting the timer ---\n",
      "--- Done: The execution took 0.6339156627655029 seconds ---\n",
      "\n",
      "Your result is correct!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "TPB = 16\n",
    "\n",
    "# Initialize the data arrays\n",
    "A = np.random.rand(TPB*100, TPB*100) # random matrix\n",
    "B = np.random.rand(TPB*100, TPB*100) # random matrix\n",
    "C = np.zeros((A.shape[0], B.shape[1])) # output matrix\n",
    "\n",
    "# Configure the blocks\n",
    "num_threads_per_block = 1\n",
    "num_blocks = (C.shape[0], C.shape[1])\n",
    "print(f\"num_blocks: {num_blocks}, num_threads_per_block: {num_threads_per_block}\")\n",
    "\n",
    "# JIT compilation/caching\n",
    "matmul[num_blocks, num_threads_per_block](A, B, C); cuda.synchronize()\n",
    "\n",
    "# --------------------------\n",
    "C = np.zeros((A.shape[0], B.shape[1])) # output matrix\n",
    "start_time = time.time()\n",
    "print(\"\\n--- [simple CUDA] Starting the timer ---\")\n",
    "# Start the kernel \n",
    "matmul[num_blocks, num_threads_per_block](A, B, C); cuda.synchronize()\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "# --------------------------\n",
    "\n",
    "# Check result with serial/CPU version\n",
    "result_serial = np.matmul(A, B)\n",
    "print(\"\\nYour result is {}\".format(\"correct!\" if np.allclose(result_serial, C) else \"incorrect.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication - Simple CUDA Solution + Different Blocks/Threads Configuration (same performance as using shared memory!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_blocks: (100, 100), num_threads_per_block: (16, 16)\n",
      "\n",
      "--- [simple CUDA] Starting the timer ---\n",
      "--- Done: The execution took 0.053615570068359375 seconds ---\n",
      "\n",
      "Your result is correct!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "TPB = 16\n",
    "\n",
    "# Initialize the data arrays\n",
    "A = np.random.rand(TPB*100, TPB*100) # random matrix\n",
    "B = np.random.rand(TPB*100, TPB*100) # random matrix\n",
    "C = np.zeros((A.shape[0], B.shape[1])) # output matrix\n",
    "\n",
    "# Configure the blocks\n",
    "num_threads_per_block = (TPB, TPB)\n",
    "blocks_x = int(math.ceil(A.shape[0] / num_threads_per_block[0]))\n",
    "blocks_y = int(math.ceil(B.shape[1] / num_threads_per_block[1]))\n",
    "num_blocks = (blocks_x, blocks_y)\n",
    "print(f\"num_blocks: {num_blocks}, num_threads_per_block: {num_threads_per_block}\")\n",
    "\n",
    "# JIT compilation/caching\n",
    "matmul[num_blocks, num_threads_per_block](A, B, C); cuda.synchronize()\n",
    "\n",
    "# --------------------------\n",
    "C = np.zeros((A.shape[0], B.shape[1])) # output matrix\n",
    "start_time = time.time()\n",
    "print(\"\\n--- [simple CUDA] Starting the timer ---\")\n",
    "# Start the kernel \n",
    "matmul[num_blocks, num_threads_per_block](A, B, C); cuda.synchronize()\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "# --------------------------\n",
    "\n",
    "# Check result with serial/CPU version\n",
    "result_serial = np.matmul(A, B)\n",
    "print(\"\\nYour result is {}\".format(\"correct!\" if np.allclose(result_serial, C) else \"incorrect.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** It seems that higher threads-per-block (16**2=256 vs. 1) and lower block count (20000 vs. 5120000) performs better **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 6] Matrix multiplication - Simple CUDA Solution + Shared CUDA Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, float32, float64\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "# TPB should not be larger than 32 in this example\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B using CUDA shared memory.\n",
    "\n",
    "    Reference: https://stackoverflow.com/a/64198479/13697228 by @RobertCrovella\n",
    "    \"\"\"\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x    # blocks per grid\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = float32(0.)\n",
    "    for i in range(bpg):\n",
    "        # Preload data into shared memory\n",
    "        sA[ty, tx] = 0\n",
    "        sB[ty, tx] = 0\n",
    "        if y < A.shape[0] and (tx + i * TPB) < A.shape[1]:\n",
    "            sA[ty, tx] = A[y, tx + i * TPB]\n",
    "        if x < B.shape[1] and (ty + i * TPB) < B.shape[0]:\n",
    "            sB[ty, tx] = B[ty + i * TPB, x]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "    if y < C.shape[0] and x < C.shape[1]:\n",
    "        C[y, x] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_blocks: (100, 100), num_threads_per_block: (16, 16)\n",
      "\n",
      "--- [simple CUDA + shared memory] Starting the timer ---\n",
      "--- Done: The execution took 0.024066925048828125 seconds ---\n",
      "\n",
      "Your result is correct!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/BIOE488/FA24/conda/rapids/lib/python3.12/site-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# Controls threads per block and shared memory usage.\n",
    "\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "#TODO - larger tiling - 256 tiling for 1024 matrix\n",
    "\n",
    "# Initialize the data arrays\n",
    "A = np.random.rand(TPB*100, TPB*100) # random matrix\n",
    "B = np.random.rand(TPB*100, TPB*100) # random matrix\n",
    "C = np.zeros((A.shape[0], B.shape[1])) # output matrix\n",
    "\n",
    "# Configure the blocks\n",
    "num_threads_per_block = (TPB, TPB)\n",
    "blocks_x = int(math.ceil(A.shape[0] / num_threads_per_block[1]))\n",
    "blocks_y = int(math.ceil(B.shape[1] / num_threads_per_block[0]))\n",
    "num_blocks = (blocks_x, blocks_y)\n",
    "print(f\"num_blocks: {num_blocks}, num_threads_per_block: {num_threads_per_block}\")\n",
    "\n",
    "# JIT compilation/caching\n",
    "fast_matmul[num_blocks, num_threads_per_block](A, B, C); cuda.synchronize()\n",
    "\n",
    "# --------------------------\n",
    "C = np.zeros((A.shape[0], B.shape[1])) # output matrix\n",
    "start_time = time.time()\n",
    "print(\"\\n--- [simple CUDA + shared memory] Starting the timer ---\")\n",
    "# Start the kernel \n",
    "fast_matmul[num_blocks, num_threads_per_block](A, B, C); cuda.synchronize()\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "# --------------------------\n",
    "\n",
    "# Check result with serial/CPU version\n",
    "result_serial = np.matmul(A, B)\n",
    "print(\"\\nYour result is {}\".format(\"correct!\" if np.allclose(result_serial, C) else \"incorrect.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "353902f3f2f769574ee6d5e609f500cb3c8385ac61494244183cc0b6ad3e28b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
