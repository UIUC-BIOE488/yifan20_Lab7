{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing CUDA Kernels\n",
    "-------------------\n",
    "## Contents\n",
    "- Example - computing pairwise distances given a dataset of points\n",
    "    - Solution using serial for-loops\n",
    "    - Simple CUDA solution\n",
    "    - Optimized CUDA solution\n",
    "- Example - matrix multiplication\n",
    "    - Simple CUDA solution\n",
    "    - Optimized CUDA solution\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the GPU allocated to us using the ```nvidia-smi``` shell command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, cuda, vectorize, guvectorize, stencil\n",
    "from numba import prange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import convolve as scipy_convolve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba.core.errors import NumbaPerformanceWarning\n",
    "# import warnings\n",
    "# warnings.simplefilter('ignore', category=NumbaPerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 1] Example - Computing Pairwise Distances\n",
    "\n",
    "This computation shows up in many practical scenarios/problems:\n",
    "- Distance matrices for clustering algorithms\n",
    "- k-Nearest Neighbors (kNN) classification algorithm\n",
    "- Estimation of topological manifolds\n",
    "- Similarity-based searches such as recommendations or information retrieval\n",
    "\n",
    "![](./distance_matrix.jpg)\n",
    "\n",
    "Notice that:\n",
    "- The cells of the distance matrix can be filled in parallel since the distance computation (Euclidean distance, for example) has no external dependencies\n",
    "- Typically, the distance between A and B is the same as B and A. Let's ignore this property for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "xy_coordinates = np.random.rand(n_samples, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 2] Serial Solution using For-loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def distance_matrix_serial(data):\n",
    "    distance_matrix = np.zeros((data.shape[0], data.shape[0]))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[0]):\n",
    "            distance_matrix[i, j] = distance.euclidean(data[i], data[j])\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"--- [Serial] Starting the timer ---\")\n",
    "result_serial = distance_matrix_serial(xy_coordinates)\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 3] Simple CUDA Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# one block computes one cell inside the distance matrix\n",
    "# single thread inside the block performs the computation\n",
    "\n",
    "@cuda.jit\n",
    "def distance_matrix_cuda(data, distance_matrix):\n",
    "\n",
    "    i, j = cuda.grid(2)\n",
    "\n",
    "    if i > len(data) or j > len(data):\n",
    "        return\n",
    "    \n",
    "    distance_matrix[i, j] = ((data[i,0] - data[j,0])**2 + (data[i,1] - data[j,1])**2)**0.5\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "cuda.select_device(2)  # Change to the device ID you want to use\n",
    "\n",
    "num_blocks = (n_samples, n_samples)\n",
    "num_threads_per_block = 1\n",
    "\n",
    "result_cuda = np.zeros((n_samples, n_samples), dtype=float)\n",
    "distance_matrix_cuda[num_blocks, num_threads_per_block](xy_coordinates, result_cuda); cuda.synchronize()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"\\n--- [simple CUDA] Starting the timer ---\")\n",
    "result_cuda = np.zeros((n_samples, n_samples), dtype=float)\n",
    "distance_matrix_cuda[num_blocks, num_threads_per_block](xy_coordinates, result_cuda); cuda.synchronize()\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"\\nYour result is {}\".format(\"correct!\" if np.allclose(result_serial, result_cuda) else \"incorrect.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 4] Simple CUDA Solution + Shared CUDA Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one block computes distances from current point to all other points\n",
    "# each point is handled by a separate thread inside the block\n",
    "# threads inside the block take the current point from shared memory of the block\n",
    "\n",
    "from numba import cuda, float64\n",
    "\n",
    "@cuda.jit\n",
    "def distance_matrix_cuda_optimized(data, distance_matrix):\n",
    "\n",
    "    current_point = cuda.shared.array(shape=(2), dtype=float64)\n",
    "\n",
    "    i = cuda.blockIdx.x\n",
    "    if i < data.shape[0]:\n",
    "        current_point = data[i]\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "\n",
    "    j = cuda.threadIdx.x\n",
    "    if j < data.shape[0]:\n",
    "        distance_matrix[i, j] = ((current_point[0] - data[j,0])**2 + (current_point[1] - data[j,1])**2) ** 0.5\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "num_blocks = n_samples\n",
    "num_threads_per_block = n_samples\n",
    "\n",
    "result_cuda_optimized = np.zeros((n_samples, n_samples), dtype=float)\n",
    "distance_matrix_cuda_optimized[num_blocks, num_threads_per_block](xy_coordinates, result_cuda_optimized); cuda.synchronize()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"\\n--- [simple CUDA + shared memory] Starting the timer ---\")\n",
    "result_cuda_optimized = np.zeros((n_samples, n_samples), dtype=float)\n",
    "distance_matrix_cuda_optimized[num_blocks, num_threads_per_block](xy_coordinates, result_cuda_optimized); cuda.synchronize()\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"\\nYour result is {}\".format(\"correct!\" if np.allclose(result_serial, result_cuda_optimized) else \"incorrect.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the difference with and without shared memory more clearly (100 repeats)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_simple_cuda(n_samples=1000, n_repeats=100):\n",
    "    exec_times = []\n",
    "    for _ in range(n_repeats):\n",
    "        xy_coordinates = np.random.rand(n_samples, 2)\n",
    "        num_blocks = (n_samples, n_samples)\n",
    "        num_threads_per_block = 1\n",
    "        result_cuda = np.zeros((n_samples, n_samples), dtype=float)\n",
    "        start_time = time.time()\n",
    "        distance_matrix_cuda[num_blocks, num_threads_per_block](xy_coordinates, result_cuda); cuda.synchronize()\n",
    "        exec_times.append(time.time() - start_time)\n",
    "    return exec_times\n",
    "\n",
    "def repeat_optimized_cuda(n_samples=1000, n_repeats=100):\n",
    "    exec_times = []\n",
    "    for _ in range(n_repeats):\n",
    "        xy_coordinates = np.random.rand(n_samples, 2)\n",
    "        num_blocks = n_samples\n",
    "        num_threads_per_block = n_samples\n",
    "        result_cuda_optimized = np.zeros((n_samples, n_samples), dtype=float)\n",
    "        start_time = time.time()\n",
    "        distance_matrix_cuda_optimized[num_blocks, num_threads_per_block](xy_coordinates, result_cuda_optimized); cuda.synchronize()\n",
    "        exec_times.append(time.time() - start_time)\n",
    "    return exec_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_times_simple_cuda = repeat_simple_cuda()\n",
    "exec_times_optimized_cuda = repeat_optimized_cuda()\n",
    "\n",
    "print(f\"Simple CUDA (100 repeats): {np.mean(exec_times_simple_cuda)} ({np.std(exec_times_simple_cuda)})\")\n",
    "print(f\"Optimized CUDA (100 repeats): {np.mean(exec_times_optimized_cuda)} ({np.std(exec_times_optimized_cuda)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 5] Matrix multiplication - Simple CUDA Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each thread fills in one cell of the output matrix C\n",
    "\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]): # or range(B.shape[1])\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "TPB = 16\n",
    "\n",
    "# Initialize the data arrays\n",
    "A = np.random.rand(TPB*100, TPB*100) # random matrix\n",
    "B = np.random.rand(TPB*100, TPB*100) # random matrix\n",
    "C = np.zeros((A.shape[0], B.shape[1])) # output matrix\n",
    "\n",
    "# Configure the blocks\n",
    "num_threads_per_block = 1\n",
    "num_blocks = (C.shape[0], C.shape[1])\n",
    "print(f\"num_blocks: {num_blocks}, num_threads_per_block: {num_threads_per_block}\")\n",
    "\n",
    "# JIT compilation/caching\n",
    "matmul[num_blocks, num_threads_per_block](A, B, C); cuda.synchronize()\n",
    "\n",
    "# --------------------------\n",
    "C = np.zeros((A.shape[0], B.shape[1])) # output matrix\n",
    "start_time = time.time()\n",
    "print(\"\\n--- [simple CUDA] Starting the timer ---\")\n",
    "# Start the kernel \n",
    "matmul[num_blocks, num_threads_per_block](A, B, C); cuda.synchronize()\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "# --------------------------\n",
    "\n",
    "# Check result with serial/CPU version\n",
    "result_serial = np.matmul(A, B)\n",
    "print(\"\\nYour result is {}\".format(\"correct!\" if np.allclose(result_serial, C) else \"incorrect.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication - Simple CUDA Solution + Different Blocks/Threads Configuration (same performance as using shared memory!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "TPB = 16\n",
    "\n",
    "# Initialize the data arrays\n",
    "A = np.random.rand(TPB*100, TPB*100) # random matrix\n",
    "B = np.random.rand(TPB*100, TPB*100) # random matrix\n",
    "C = np.zeros((A.shape[0], B.shape[1])) # output matrix\n",
    "\n",
    "# Configure the blocks\n",
    "num_threads_per_block = (TPB, TPB)\n",
    "blocks_x = int(math.ceil(A.shape[0] / num_threads_per_block[0]))\n",
    "blocks_y = int(math.ceil(B.shape[1] / num_threads_per_block[1]))\n",
    "num_blocks = (blocks_x, blocks_y)\n",
    "print(f\"num_blocks: {num_blocks}, num_threads_per_block: {num_threads_per_block}\")\n",
    "\n",
    "# JIT compilation/caching\n",
    "matmul[num_blocks, num_threads_per_block](A, B, C); cuda.synchronize()\n",
    "\n",
    "# --------------------------\n",
    "C = np.zeros((A.shape[0], B.shape[1])) # output matrix\n",
    "start_time = time.time()\n",
    "print(\"\\n--- [simple CUDA] Starting the timer ---\")\n",
    "# Start the kernel \n",
    "matmul[num_blocks, num_threads_per_block](A, B, C); cuda.synchronize()\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "# --------------------------\n",
    "\n",
    "# Check result with serial/CPU version\n",
    "result_serial = np.matmul(A, B)\n",
    "print(\"\\nYour result is {}\".format(\"correct!\" if np.allclose(result_serial, C) else \"incorrect.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** It seems that higher threads-per-block (16**2=256 vs. 1) and lower block count (20000 vs. 5120000) performs better **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 6] Matrix multiplication - Simple CUDA Solution + Shared CUDA Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, float32, float64\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "# TPB should not be larger than 32 in this example\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B using CUDA shared memory.\n",
    "\n",
    "    Reference: https://stackoverflow.com/a/64198479/13697228 by @RobertCrovella\n",
    "    \"\"\"\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x    # blocks per grid\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = float32(0.)\n",
    "    for i in range(bpg):\n",
    "        # Preload data into shared memory\n",
    "        sA[ty, tx] = 0\n",
    "        sB[ty, tx] = 0\n",
    "        if y < A.shape[0] and (tx + i * TPB) < A.shape[1]:\n",
    "            sA[ty, tx] = A[y, tx + i * TPB]\n",
    "        if x < B.shape[1] and (ty + i * TPB) < B.shape[0]:\n",
    "            sB[ty, tx] = B[ty + i * TPB, x]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "    if y < C.shape[0] and x < C.shape[1]:\n",
    "        C[y, x] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controls threads per block and shared memory usage.\n",
    "\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "#TODO - larger tiling - 256 tiling for 1024 matrix\n",
    "\n",
    "# Initialize the data arrays\n",
    "A = np.random.rand(TPB*100, TPB*100) # random matrix\n",
    "B = np.random.rand(TPB*100, TPB*100) # random matrix\n",
    "C = np.zeros((A.shape[0], B.shape[1])) # output matrix\n",
    "\n",
    "# Configure the blocks\n",
    "num_threads_per_block = (TPB, TPB)\n",
    "blocks_x = int(math.ceil(A.shape[0] / num_threads_per_block[1]))\n",
    "blocks_y = int(math.ceil(B.shape[1] / num_threads_per_block[0]))\n",
    "num_blocks = (blocks_x, blocks_y)\n",
    "print(f\"num_blocks: {num_blocks}, num_threads_per_block: {num_threads_per_block}\")\n",
    "\n",
    "# JIT compilation/caching\n",
    "fast_matmul[num_blocks, num_threads_per_block](A, B, C); cuda.synchronize()\n",
    "\n",
    "# --------------------------\n",
    "C = np.zeros((A.shape[0], B.shape[1])) # output matrix\n",
    "start_time = time.time()\n",
    "print(\"\\n--- [simple CUDA + shared memory] Starting the timer ---\")\n",
    "# Start the kernel \n",
    "fast_matmul[num_blocks, num_threads_per_block](A, B, C); cuda.synchronize()\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "# --------------------------\n",
    "\n",
    "# Check result with serial/CPU version\n",
    "result_serial = np.matmul(A, B)\n",
    "print(\"\\nYour result is {}\".format(\"correct!\" if np.allclose(result_serial, C) else \"incorrect.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "353902f3f2f769574ee6d5e609f500cb3c8385ac61494244183cc0b6ad3e28b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
